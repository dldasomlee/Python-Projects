{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data\n",
    "1. Filter on 'submission_time', 'upvotes', 'url', 'headline'. Let's use 'headline' to predict the number of 'upvotes' the articles received to find popular articles. \n",
    "2. Let's train a linear regression model to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Choose four columns\n",
    "import pandas as pd\n",
    "submissions = pd.read_csv(\"stories.csv\")\n",
    "submissions.columns = [\"submission_time\", \"upvotes\", \"url\", \"headline\"]\n",
    "submissions = submissions.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "1. My big goal is to convert each headline to a numerical representation. To do this, let's break a sentence into disconnected words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split each headlines\n",
    "tokenized_headlines = []\n",
    "for item in submissions[\"headline\"]:\n",
    "    tokenized_headlines.append(item.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "1. To make the prediction accurate, help the computer group the same word together, such as group the words that's only different by lower/upper case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punctuation = [\",\", \":\", \";\", \".\", \"'\", '\"', \"â€™\", \"?\", \"/\", \"-\", \"+\", \"&\", \"(\", \")\"]\n",
    "clean_tokenized = []\n",
    "for item in tokenized_headlines:\n",
    "    tokens = []\n",
    "    # convert tokens to lower case\n",
    "    for token in item:\n",
    "        token = token.lower()\n",
    "        # remove punctuation\n",
    "        for punc in punctuation:\n",
    "            token = token.replace(punc, \"\")\n",
    "        #append the cleaned list\n",
    "        tokens.append(token)\n",
    "    clean_tokenized.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to numerical representation\n",
    "1. Grab all the unique words from the headlines and make them the column headers of a matrix. Initialize all the values in the matrix to 0.\n",
    "2. Try a Pandas Dataframe so that it can create rows with zero values. The length will be equl to the 'clean_tokenized' list and each column will be from 'unique_tokens'.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Find the unique tokens\n",
    "unique_tokens = []\n",
    "single_tokens = []\n",
    "for tokens in clean_tokenized:\n",
    "    for token in tokens:\n",
    "        # keep tokens that occur only once\n",
    "        if token not in single_tokens:\n",
    "            single_tokens.append(token)\n",
    "        # keep tokens that occur at least twice\n",
    "        elif token in single_tokens and token not in unique_tokens:\n",
    "            unique_tokens.append(token)\n",
    "# Dataframe with all zero values with each column being a token in 'unique_tokens'\n",
    "counts = pd.DataFrame(0, index=np.arange(len(clean_tokenized)), columns=unique_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enumerate tokens\n",
    "1. Now that each row is filled with 0 values, let's count how many times each token occured in the headline. To do this, let's loop through each list of tokens in 'clean_tokenized' and increment the column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the column for the token\n",
    "for i, item in enumerate(clean_tokenized):\n",
    "    for token in item:\n",
    "        if token in unique_tokens:\n",
    "            counts.iloc[i][token] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove extra\n",
    "1. Since there are 2,000+ columns, this can make a linear regression hard to predict. To improve prediction accuracy, let's reduce words that occur fewer than 5 times or occur more than 100 times. By doing this, the number of features that include 'and,' 'to,' and any words that appear only a few times will be reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate vetor for sums of columns in counts\n",
    "word_counts = counts.sum(axis=0)\n",
    "# Use loc to filter counts (less than 5 or more than 100)\n",
    "counts = counts.loc[:,(word_counts >= 5) & (word_counts <= 100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data\n",
    "1. Let's split data to test the efficacy of algorithm. One will be on a test set and the other will be on a train set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use train_test_split function\n",
    "from sklearn.cross_validation import train_test_split\n",
    "# 20% of rows for testing and 80% for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(counts, submissions[\"upvotes\"], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions\n",
    "1. Use a lingear regression algorithm from scikit-learn to make predictions on test set. To do this, use the 'fit' model on the training set with 'X_train' and 'y_train'. After this, make predictions using 'X_test'. By doing this, it will find correlations between upvotes and the assigned coefficients in the column, which will help predict popular headlines with many upvotes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Train clf using the fit method\n",
    "clf = LinearRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "# Predictions on X_test\n",
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error\n",
    "1. Let's calculate error on the prediction. Use mean squred error to have predictions relatively close to the actual values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2323.66357839\n"
     ]
    }
   ],
   "source": [
    "## Use MSE\n",
    "## Calculate the mean squared error \n",
    "mse = sum((y_test - predictions) ** 2) / len(predictions)\n",
    "print(mse)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
